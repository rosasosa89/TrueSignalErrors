import numpy as np
import matplotlib.pyplot as plt
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Dense
from tensorflow.keras.optimizers import Adam
from sklearn.metrics import mean_squared_error

# Simulate synthetic phase error data (as a placeholder for actual data)
def generate_synthetic_data(num_samples, sequence_length):
    """
    Generates synthetic data for phase error estimation task.
    """
    X = np.random.randn(num_samples, sequence_length, 1)  # Random features
    y = np.sin(np.linspace(0, 2 * np.pi, num_samples)) + np.random.normal(0, 0.1, num_samples)  # Sinusoidal target with noise
    return X, y

# Train LSTM model with different architectures
def train_lstm_model(X_train, y_train, sequence_length, hidden_size):
    """
    Train an LSTM model with a given sequence length and hidden size.
    """
    model = Sequential()
    model.add(LSTM(hidden_size, input_shape=(sequence_length, 1), activation='tanh'))
    model.add(Dense(1))  # Single output for phase estimation
    
    model.compile(optimizer=Adam(), loss='mean_squared_error')
    
    # Train the model
    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)
    
    # Get predictions and compute the variance of estimation
    y_pred = model.predict(X_train)
    estimation_variance = mean_squared_error(y_train, y_pred)
    
    return estimation_variance, model

# Evaluate different architectures
def evaluate_architectures(sequence_lengths, hidden_sizes, num_samples=1000):
    """
    Evaluate multiple LSTM architectures varying sequence length and hidden size,
    and report estimation variance and model complexity (scale).
    """
    results = []
    
    # Generate synthetic training data
    for seq_len in sequence_lengths:
        X_train, y_train = generate_synthetic_data(num_samples, seq_len)
        
        for hidden_size in hidden_sizes:
            # Train the model
            variance, model = train_lstm_model(X_train, y_train, seq_len, hidden_size)
            
            # Calculate model scale: S = N * zdim
            scale = seq_len * hidden_size
            
            # Record results
            results.append((seq_len, hidden_size, scale, variance))
    
    return results

# Define the sequence lengths and hidden sizes to evaluate
sequence_lengths = [10, 50, 100, 200]  # Different sequence lengths (N)
hidden_sizes = [16, 32, 64, 128]       # Different hidden sizes (zdim)

# Evaluate architectures
results = evaluate_architectures(sequence_lengths, hidden_sizes)

# Convert results to a numpy array for easier handling
results = np.array(results)

# Plot the results: Estimation variance vs model complexity (scale)
plt.figure(figsize=(10, 6))

# We will plot variance against scale (N * zdim)
for seq_len in sequence_lengths:
    seq_len_results = results[results[:, 0] == seq_len]
    plt.plot(seq_len_results[:, 2], seq_len_results[:, 3], label=f"Seq Len = {seq_len}")

plt.xlabel('Model Complexity (Scale: N * zdim)')
plt.ylabel('Estimation Variance (MSE)')
plt.title('Estimation Variance vs Model Complexity for Different LSTM Architectures')
plt.legend()
plt.grid(True)
plt.show()

# Print the results (for verification)
for res in results:
    print(f"Seq Len: {int(res[0])}, Hidden Size: {int(res[1])}, Scale: {int(res[2])}, Variance: {res[3]:.6f}")
